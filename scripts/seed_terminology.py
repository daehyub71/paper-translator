"""
ì´ˆê¸° ì „ë¬¸ìš©ì–´ ë°ì´í„° ì‚½ì… ìŠ¤í¬ë¦½íŠ¸
AI/ML ë¶„ì•¼ ê¸°ë³¸ ìš©ì–´ë¥¼ terminology_mappings í…Œì´ë¸”ì— ì‚½ì…
"""
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from supabase import create_client, Client

load_dotenv()


# ì´ˆê¸° ìš©ì–´ ë°ì´í„°
SEED_TERMS = [
    # ============================================
    # Architecture (ì•„í‚¤í…ì²˜)
    # ============================================
    {"source_text": "Transformer", "target_text": "íŠ¸ëœìŠ¤í¬ë¨¸", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "attention mechanism", "target_text": "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "self-attention", "target_text": "ì…€í”„ ì–´í…ì…˜", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "multi-head attention", "target_text": "ë©€í‹°í—¤ë“œ ì–´í…ì…˜", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "feed-forward network", "target_text": "í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "residual connection", "target_text": "ì”ì°¨ ì—°ê²°", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "layer normalization", "target_text": "ë ˆì´ì–´ ì •ê·œí™”", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "encoder", "target_text": "ì¸ì½”ë”", "mapping_type": "word", "domain": "General"},
    {"source_text": "decoder", "target_text": "ë””ì½”ë”", "mapping_type": "word", "domain": "General"},
    {"source_text": "neural network", "target_text": "ì‹ ê²½ë§", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "deep learning", "target_text": "ë”¥ëŸ¬ë‹", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "convolutional neural network", "target_text": "í•©ì„±ê³± ì‹ ê²½ë§", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "recurrent neural network", "target_text": "ìˆœí™˜ ì‹ ê²½ë§", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "LSTM", "target_text": "LSTM", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "GRU", "target_text": "GRU", "mapping_type": "word", "domain": "NLP"},

    # ============================================
    # Training (í•™ìŠµ)
    # ============================================
    {"source_text": "fine-tuning", "target_text": "ë¯¸ì„¸ì¡°ì •", "mapping_type": "word", "domain": "General"},
    {"source_text": "pre-training", "target_text": "ì‚¬ì „í•™ìŠµ", "mapping_type": "word", "domain": "General"},
    {"source_text": "transfer learning", "target_text": "ì „ì´í•™ìŠµ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "gradient descent", "target_text": "ê²½ì‚¬ í•˜ê°•ë²•", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "stochastic gradient descent", "target_text": "í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "backpropagation", "target_text": "ì—­ì „íŒŒ", "mapping_type": "word", "domain": "General"},
    {"source_text": "learning rate", "target_text": "í•™ìŠµë¥ ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "batch size", "target_text": "ë°°ì¹˜ í¬ê¸°", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "epoch", "target_text": "ì—í­", "mapping_type": "word", "domain": "General"},
    {"source_text": "overfitting", "target_text": "ê³¼ì í•©", "mapping_type": "word", "domain": "General"},
    {"source_text": "underfitting", "target_text": "ê³¼ì†Œì í•©", "mapping_type": "word", "domain": "General"},
    {"source_text": "regularization", "target_text": "ì •ê·œí™”", "mapping_type": "word", "domain": "General"},
    {"source_text": "dropout", "target_text": "ë“œë¡­ì•„ì›ƒ", "mapping_type": "word", "domain": "General"},
    {"source_text": "weight decay", "target_text": "ê°€ì¤‘ì¹˜ ê°ì‡ ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "optimizer", "target_text": "ì˜µí‹°ë§ˆì´ì €", "mapping_type": "word", "domain": "General"},
    {"source_text": "Adam", "target_text": "Adam", "mapping_type": "word", "domain": "General"},
    {"source_text": "loss function", "target_text": "ì†ì‹¤ í•¨ìˆ˜", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "cross-entropy", "target_text": "í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼", "mapping_type": "word", "domain": "General"},

    # ============================================
    # LLM Specific (ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)
    # ============================================
    {"source_text": "large language model", "target_text": "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "hallucination", "target_text": "í™˜ê° í˜„ìƒ", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "prompt engineering", "target_text": "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "in-context learning", "target_text": "ì¸ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "chain-of-thought", "target_text": "ì‚¬ê³ ì˜ ì—°ì‡„", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "retrieval-augmented generation", "target_text": "ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "RAG", "target_text": "RAG", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "tokenization", "target_text": "í† í°í™”", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "embedding", "target_text": "ì„ë² ë”©", "mapping_type": "word", "domain": "General"},
    {"source_text": "word embedding", "target_text": "ë‹¨ì–´ ì„ë² ë”©", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "positional encoding", "target_text": "ìœ„ì¹˜ ì¸ì½”ë”©", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "context window", "target_text": "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "zero-shot", "target_text": "ì œë¡œìƒ·", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "few-shot", "target_text": "í“¨ìƒ·", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "instruction tuning", "target_text": "ì§€ì‹œ íŠœë‹", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "RLHF", "target_text": "ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ(RLHF)", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "reinforcement learning from human feedback", "target_text": "ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ", "mapping_type": "phrase", "domain": "NLP"},

    # ============================================
    # Metrics (í‰ê°€ ì§€í‘œ)
    # ============================================
    {"source_text": "accuracy", "target_text": "ì •í™•ë„", "mapping_type": "word", "domain": "General"},
    {"source_text": "precision", "target_text": "ì •ë°€ë„", "mapping_type": "word", "domain": "General"},
    {"source_text": "recall", "target_text": "ì¬í˜„ìœ¨", "mapping_type": "word", "domain": "General"},
    {"source_text": "F1 score", "target_text": "F1 ì ìˆ˜", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "perplexity", "target_text": "í¼í”Œë ‰ì‹œí‹°", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "BLEU score", "target_text": "BLEU ì ìˆ˜", "mapping_type": "phrase", "domain": "NLP"},
    {"source_text": "ROUGE", "target_text": "ROUGE", "mapping_type": "word", "domain": "NLP"},
    {"source_text": "AUC", "target_text": "AUC", "mapping_type": "word", "domain": "General"},
    {"source_text": "ROC curve", "target_text": "ROC ê³¡ì„ ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "mean squared error", "target_text": "í‰ê·  ì œê³± ì˜¤ì°¨", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "MSE", "target_text": "MSE", "mapping_type": "word", "domain": "General"},

    # ============================================
    # Common Phrases (ìì£¼ ì“°ì´ëŠ” í‘œí˜„)
    # ============================================
    {"source_text": "state-of-the-art", "target_text": "ìµœì‹  ê¸°ìˆ ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "from scratch", "target_text": "ì²˜ìŒë¶€í„°", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "end-to-end", "target_text": "ì—”ë“œíˆ¬ì—”ë“œ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "out-of-the-box", "target_text": "ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "benchmark", "target_text": "ë²¤ì¹˜ë§ˆí¬", "mapping_type": "word", "domain": "General"},
    {"source_text": "baseline", "target_text": "ë² ì´ìŠ¤ë¼ì¸", "mapping_type": "word", "domain": "General"},
    {"source_text": "ablation study", "target_text": "ì ˆì œ ì—°êµ¬", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "downstream task", "target_text": "ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "upstream task", "target_text": "ì—…ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬", "mapping_type": "phrase", "domain": "General"},
    {"source_text": "scalability", "target_text": "í™•ì¥ì„±", "mapping_type": "word", "domain": "General"},
    {"source_text": "inference", "target_text": "ì¶”ë¡ ", "mapping_type": "word", "domain": "General"},
    {"source_text": "latency", "target_text": "ì§€ì—° ì‹œê°„", "mapping_type": "word", "domain": "General"},
    {"source_text": "throughput", "target_text": "ì²˜ë¦¬ëŸ‰", "mapping_type": "word", "domain": "General"},

    # ============================================
    # Computer Vision (ì»´í“¨í„° ë¹„ì „)
    # ============================================
    {"source_text": "image classification", "target_text": "ì´ë¯¸ì§€ ë¶„ë¥˜", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "object detection", "target_text": "ê°ì²´ íƒì§€", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "semantic segmentation", "target_text": "ì˜ë¯¸ë¡ ì  ë¶„í• ", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "feature extraction", "target_text": "íŠ¹ì§• ì¶”ì¶œ", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "bounding box", "target_text": "ë°”ìš´ë”© ë°•ìŠ¤", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "Vision Transformer", "target_text": "ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸", "mapping_type": "phrase", "domain": "CV"},
    {"source_text": "ViT", "target_text": "ViT", "mapping_type": "word", "domain": "CV"},

    # ============================================
    # Reinforcement Learning (ê°•í™”í•™ìŠµ)
    # ============================================
    {"source_text": "reinforcement learning", "target_text": "ê°•í™”í•™ìŠµ", "mapping_type": "phrase", "domain": "RL"},
    {"source_text": "reward", "target_text": "ë³´ìƒ", "mapping_type": "word", "domain": "RL"},
    {"source_text": "policy", "target_text": "ì •ì±…", "mapping_type": "word", "domain": "RL"},
    {"source_text": "agent", "target_text": "ì—ì´ì „íŠ¸", "mapping_type": "word", "domain": "RL"},
    {"source_text": "environment", "target_text": "í™˜ê²½", "mapping_type": "word", "domain": "RL"},
    {"source_text": "action", "target_text": "í–‰ë™", "mapping_type": "word", "domain": "RL"},
    {"source_text": "state", "target_text": "ìƒíƒœ", "mapping_type": "word", "domain": "RL"},
    {"source_text": "Q-learning", "target_text": "QëŸ¬ë‹", "mapping_type": "word", "domain": "RL"},
    {"source_text": "PPO", "target_text": "PPO", "mapping_type": "word", "domain": "RL"},
]


def get_supabase_client() -> Client:
    """Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")

    if not url or not key:
        raise ValueError("SUPABASE_URL ë˜ëŠ” SUPABASE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return create_client(url, key)


def seed_terminology(client: Client, terms: list[dict]) -> dict:
    """ìš©ì–´ ë°ì´í„° ì‚½ì…"""
    result = {
        "inserted": 0,
        "skipped": 0,
        "errors": []
    }

    for term in terms:
        try:
            # upsertë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸
            response = client.table("terminology_mappings").upsert(
                term,
                on_conflict="source_text,domain"
            ).execute()

            if response.data:
                result["inserted"] += 1
            else:
                result["skipped"] += 1

        except Exception as e:
            error_msg = f"{term['source_text']}: {str(e)}"
            result["errors"].append(error_msg)

    return result


def get_term_count(client: Client) -> dict:
    """ë„ë©”ì¸ë³„ ìš©ì–´ ìˆ˜ ì¡°íšŒ"""
    response = client.table("terminology_mappings").select("domain").execute()

    if not response.data:
        return {}

    counts = {}
    for row in response.data:
        domain = row["domain"]
        counts[domain] = counts.get(domain, 0) + 1

    return counts


def main():
    print("ğŸŒ± Paper Translator ì´ˆê¸° ìš©ì–´ ë°ì´í„° ì‚½ì…")
    print("=" * 60)

    # Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    try:
        client = get_supabase_client()
        print("âœ… Supabase ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # ì‚½ì… ì „ ìš©ì–´ ìˆ˜
    print("\nğŸ“Š ì‚½ì… ì „ ìš©ì–´ í˜„í™©:")
    counts_before = get_term_count(client)
    if counts_before:
        for domain, count in sorted(counts_before.items()):
            print(f"  - {domain}: {count}ê°œ")
    else:
        print("  - (ìš©ì–´ ì—†ìŒ)")

    total_before = sum(counts_before.values()) if counts_before else 0

    # ìš©ì–´ ë°ì´í„° ì‚½ì…
    print(f"\nğŸ“¥ {len(SEED_TERMS)}ê°œì˜ ìš©ì–´ ì‚½ì… ì¤‘...")

    result = seed_terminology(client, SEED_TERMS)

    print(f"\nğŸ“‹ ì‚½ì… ê²°ê³¼:")
    print(f"  - ì‚½ì…/ì—…ë°ì´íŠ¸: {result['inserted']}ê°œ")
    print(f"  - ê±´ë„ˆëœ€: {result['skipped']}ê°œ")
    print(f"  - ì˜¤ë¥˜: {len(result['errors'])}ê°œ")

    if result["errors"]:
        print("\nâš ï¸ ì˜¤ë¥˜ ëª©ë¡:")
        for error in result["errors"][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            print(f"  - {error}")
        if len(result["errors"]) > 5:
            print(f"  ... ì™¸ {len(result['errors']) - 5}ê°œ")

    # ì‚½ì… í›„ ìš©ì–´ ìˆ˜
    print("\nğŸ“Š ì‚½ì… í›„ ìš©ì–´ í˜„í™©:")
    counts_after = get_term_count(client)
    for domain, count in sorted(counts_after.items()):
        print(f"  - {domain}: {count}ê°œ")

    total_after = sum(counts_after.values())
    new_terms = total_after - total_before

    print("\n" + "=" * 60)
    print(f"ğŸ‰ ì™„ë£Œ! ì´ {total_after}ê°œ ìš©ì–´ ({new_terms}ê°œ ì‹ ê·œ)")


if __name__ == "__main__":
    main()
