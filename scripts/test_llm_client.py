"""
LLM í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils import get_llm_client, count_tokens, translate_text


def test_initialization():
    """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    client = get_llm_client()
    print(f"âœ… ëª¨ë¸: {client.model}")
    print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")

    return True


def test_token_counting():
    """í† í° ì¹´ìš´íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”¢ í† í° ì¹´ìš´íŠ¸ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    test_texts = [
        "Hello, world!",
        "The Transformer model uses self-attention mechanism.",
        "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
    ]

    for text in test_texts:
        tokens = count_tokens(text)
        print(f"âœ… \"{text[:30]}...\" â†’ {tokens} í† í°")

    return True


def test_translation():
    """ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    print("\nğŸŒ ë²ˆì—­ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = """
    The Transformer architecture has revolutionized natural language processing.
    It uses self-attention mechanisms to process sequential data in parallel,
    making it significantly faster than recurrent neural networks.
    """

    terminology_prompt = """
- Transformer: íŠ¸ëœìŠ¤í¬ë¨¸
- self-attention: ì…€í”„ ì–´í…ì…˜
- natural language processing: ìì—°ì–´ ì²˜ë¦¬
- recurrent neural networks: ìˆœí™˜ ì‹ ê²½ë§
"""

    print("ì›ë¬¸:")
    print(f"  {test_text.strip()[:80]}...")

    result = translate_text(test_text, terminology_prompt)

    print(f"\në²ˆì—­ë¬¸:")
    print(f"  {result['translated_text'][:100]}...")
    print(f"\ní† í° ì‚¬ìš©ëŸ‰:")
    print(f"  - ì…ë ¥: {result['input_tokens']}")
    print(f"  - ì¶œë ¥: {result['output_tokens']}")
    print(f"  - ì´í•©: {result['total_tokens']}")

    return True


def test_metadata_extraction():
    """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“‹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    # ê°„ë‹¨í•œ ë…¼ë¬¸ ì•ë¶€ë¶„ ì‹œë®¬ë ˆì´ì…˜
    sample_paper = """
    Attention Is All You Need

    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

    Abstract

    The dominant sequence transduction models are based on complex recurrent or
    convolutional neural networks that include an encoder and a decoder. The best
    performing models also connect the encoder and decoder through an attention
    mechanism. We propose a new simple network architecture, the Transformer,
    based solely on attention mechanisms, dispensing with recurrence and convolutions
    entirely.
    """

    client = get_llm_client()
    result = client.extract_paper_metadata(sample_paper)

    print(f"âœ… ì œëª©: {result.get('title', 'N/A')}")
    print(f"âœ… í•œêµ­ì–´ ì œëª©: {result.get('title_ko', 'N/A')}")
    print(f"âœ… ì €ì: {', '.join(result.get('authors', [])[:3])}...")
    print(f"âœ… ë„ë©”ì¸: {result.get('domain', 'N/A')}")

    return True


def main():
    print("ğŸ§ª LLM í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    results = {
        "ì´ˆê¸°í™”": test_initialization(),
        "í† í° ì¹´ìš´íŠ¸": test_token_counting(),
        "ë²ˆì—­": test_translation(),
        "ë©”íƒ€ë°ì´í„° ì¶”ì¶œ": test_metadata_extraction(),
    }

    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… í†µê³¼" if passed else "âŒ ì‹¤íŒ¨"
        print(f"  {test_name}: {status}")
        if not passed:
            all_passed = False

    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ ëª¨ë“  LLM í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
